{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add weighted loss\n",
    "# TODO: logits scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HTTP_PROXY'] = \"http://prusaivabot:T3!-GE8k@proxyru-rd.huawei.com:8080\"\n",
    "os.environ['HTTPS_PROXY'] = \"http://prusaivabot:T3!-GE8k@proxyru-rd.huawei.com:8080\"\n",
    "os.environ['CURL_CA_BUNDLE'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1588677652.2177637"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# !pip install efficientnet_pytorch\n",
    "# !pip install albumentations torchvision\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose,\n",
    ")\n",
    "import albumentations\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 1e-5\n",
    "save_path = './saved_models/'\n",
    "batch_size = 10\n",
    "cover_data_path = '/media/vlad/hdd4_tb/datasets/alaska2/Cover/'\n",
    "test_data_path = '/media/vlad/hdd4_tb/datasets/alaska2/Test'\n",
    "base_data_path = '/media/vlad/hdd4_tb/datasets/alaska2/'\n",
    "device = torch.device('cuda:0')\n",
    "log_every = 100\n",
    "target_metric = 'weighted_auc'\n",
    "model_name = 'efficientnet-b1'\n",
    "submission_path = 'submissions/submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orig_imgs(path):\n",
    "    return os.listdir(path)\n",
    "\n",
    "def get_negative_examples(path):\n",
    "    return [os.path.join(path, img) for img in get_orig_imgs(path)]\n",
    "\n",
    "def get_positive_examples(base_path, base_data_path:str, orig_images: List[str]):\n",
    "    folders = ['JMiPOD', 'JUNIWARD', 'UERD']    \n",
    "    positive_images = []\n",
    "    \n",
    "    for folder in folders:\n",
    "        for img in orig_images:\n",
    "            positive_images.append(os.path.join(base_path, folder, img))\n",
    "    \n",
    "    return positive_images\n",
    "\n",
    "def strong_aug(p=0.5):\n",
    "    return albumentations.Compose([\n",
    "        RandomRotate90(),\n",
    "        Flip(),\n",
    "        Transpose(),\n",
    "        OneOf([\n",
    "            IAAAdditiveGaussianNoise(),\n",
    "            GaussNoise(),\n",
    "        ], p=0.2),\n",
    "        OneOf([\n",
    "            MotionBlur(p=0.2),\n",
    "            MedianBlur(blur_limit=3, p=0.1),\n",
    "            Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.2),\n",
    "        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n",
    "        OneOf([\n",
    "            OpticalDistortion(p=0.3),\n",
    "            GridDistortion(p=0.1),\n",
    "            IAAPiecewiseAffine(p=0.3),\n",
    "        ], p=0.2),\n",
    "        OneOf([\n",
    "            CLAHE(clip_limit=2),\n",
    "            IAASharpen(),\n",
    "            IAAEmboss(),\n",
    "            RandomBrightnessContrast(),\n",
    "        ], p=0.3),\n",
    "        HueSaturationValue(p=0.3),\n",
    "    ], p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths: List[str], labels: np.array, augs=None):\n",
    "        self.labels = labels\n",
    "        self.image_paths = image_paths\n",
    "        \n",
    "        self.tfms = transforms.Compose([transforms.Resize(512), \n",
    "                                        transforms.ToTensor()])\n",
    "        if augs is not None:\n",
    "            self.augs = augs()\n",
    "        else:\n",
    "            self.augs = augs\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        img = self.image_paths[idx]\n",
    "        image = plt.imread(img)\n",
    "        image = Image.fromarray(image).convert('RGB')\n",
    "        \n",
    "        if self.augs is not None:\n",
    "            image = self.augs(image=np.array(image))['image']\n",
    "        else:\n",
    "            image = np.array(image)\n",
    "        \n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        \n",
    "        return {'X': torch.tensor(image), 'Y': label}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_imgs = get_orig_imgs(cover_data_path)\n",
    "test_imgs = get_negative_examples(test_data_path)\n",
    "\n",
    "negatives = get_negative_examples(cover_data_path)\n",
    "positives = get_positive_examples(base_data_path, cover_data_path, orig_imgs)\n",
    "train_paths = negatives + positives\n",
    "train_labels = [1] * len(positives) + [0] * len(negatives)\n",
    "\n",
    "train_paths, valid_paths, train_labels, valid_labels = train_test_split(\n",
    "    train_paths, train_labels, test_size=0.15, random_state=2020)\n",
    "\n",
    "train_paths = train_paths[:30]\n",
    "train_labels = train_labels[:30]\n",
    "valid_paths = valid_paths[:30]\n",
    "valid_labels = valid_labels[:30]\n",
    "\n",
    "train_dataset = ImageDataset(train_paths, train_labels, strong_aug)\n",
    "valid_dataset = ImageDataset(valid_paths, valid_labels, strong_aug)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetClassifier(torch.nn.Module):\n",
    "    def __init__(self, efficient_net_name):\n",
    "        super(EfficientNetClassifier, self).__init__()\n",
    "        self.backbone = EfficientNet.from_pretrained(efficient_net_name, num_classes=1)\n",
    "    \n",
    "    def forward(self, batch: torch.Tensor) -> torch.tensor:\n",
    "        # 3, 512, 512\n",
    "        return self.backbone.forward(batch)\n",
    "\n",
    "def evaluate_model(val_dataloader: DataLoader, classifier: torch.nn.Module, criterion):\n",
    "    epoch_valid_loss = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            x = batch['X'].to(device)\n",
    "            labels = batch['Y'].to(device)\n",
    "\n",
    "            logits = classifier.forward(x)\n",
    "            loss = criterion(logits, labels.float().unsqueeze(dim=1))\n",
    "            probs = torch.sigmoid(logits.squeeze())\n",
    "            \n",
    "            epoch_valid_loss.append(loss.item())\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "            \n",
    "    loss = np.mean(epoch_valid_loss)\n",
    "    return all_probs, all_labels, loss\n",
    "\n",
    "\n",
    "def weighted_auc(labels: List[int], preds: List[float], plot = False):\n",
    "    tpr_thresholds = [0.0, 0.4, 1.0]\n",
    "    weights = [2, 1]\n",
    "    \n",
    "    # Calculating ROC curve\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, preds, pos_label=1)\n",
    "    # data labels, preds\n",
    "    area = np.array(tpr_thresholds)[1:] - np.array(tpr_thresholds)[:-1]\n",
    "    area_normalized = np.dot(area, np.array(weights).T)  # For normalizing AUC\n",
    "    fscore = 0\n",
    "    for index, weight in enumerate(weights):\n",
    "        ymin = tpr_thresholds[index]    \n",
    "        ymax = tpr_thresholds[index + 1]\n",
    "\n",
    "        mask = (tpr > ymin) & (tpr < ymax)\n",
    "        try:\n",
    "            x = np.concatenate([fpr[mask], np.linspace(fpr[mask][-1], 1, 100)])\n",
    "        except Exception as e:\n",
    "            print(fpr)\n",
    "            print(mask)\n",
    "            raise e\n",
    "        y = np.concatenate([tpr[mask], [ymax] * 100])\n",
    "        y = y #(taking y as origin)\n",
    "        score = metrics.auc(x, y-ymin)\n",
    "        # Multiply score with weight\n",
    "        weighted_score = score * weight\n",
    "\n",
    "        fscore += weighted_score\n",
    "        color = [\"red\", \"green\"]\n",
    "        label = [\"x ∈ [0, 1], y ∈ [0, 0.4]\", \"x ∈ [0, 1], y ∈ [0.4, 1.0]\"]\n",
    "        \n",
    "        if plot:\n",
    "            plt.title(\"Separate plots for x ∈ [0, 1], y ∈ [0, 0.4] and x ∈ [0, 1], y ∈ [0.4, 1.0]\")\n",
    "            plt.plot(x, y, color = color[index], label = label[index])\n",
    "            plt.xlabel(\"False Positive rate\")\n",
    "            plt.ylabel(\"True Positive rate\")\n",
    "            plt.legend(loc = 2)\n",
    "#             plt.plot()\n",
    "\n",
    "    # Normalizing score\n",
    "    final_score = fscore/area_normalized\n",
    "    return final_score\n",
    "\n",
    "        \n",
    "    return competition_metric / normalization\n",
    "\n",
    "def make_submission(model: torch.nn.Module, test_data_path: str, batch_size: int, device: torch.device) -> pd.DataFrame:\n",
    "    \n",
    "    images = get_negative_examples(test_data_path)\n",
    "    \n",
    "    dataset = ImageDataset(images, [0] * len(images))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.ones(1)).to(device)\n",
    "    \n",
    "    all_probs, all_labels, loss = evaluate_model(dataloader, model, criterion)\n",
    "    all_preds = (torch.tensor(all_probs, dtype=torch.float32) > 0.5).long()\n",
    "    \n",
    "    images_names = get_orig_imgs(test_data_path)\n",
    "    return pd.DataFrame({'Id': images_names, 'Label': all_preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b1\n"
     ]
    }
   ],
   "source": [
    "classifier = EfficientNetClassifier('efficientnet-b1').to(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.ones(1)).to(device)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 0/10\n",
      "Epoch: 0/10\n",
      "Batch 0/3: bce_loss: 0.622189998626709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 1/10\n",
      "Epoch: 1/10\n",
      "Batch 0/3: bce_loss: 0.6337394714355469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 2/10\n",
      "Epoch: 2/10\n",
      "Batch 0/3: bce_loss: 0.6695546507835388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 3/10\n",
      "Epoch: 3/10\n",
      "Batch 0/3: bce_loss: 0.6388421058654785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 4/10\n",
      "Epoch: 4/10\n",
      "Batch 0/3: bce_loss: 0.631355881690979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 5/10\n",
      "Epoch: 5/10\n",
      "Batch 0/3: bce_loss: 0.6342142224311829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 6/10\n",
      "Epoch: 6/10\n",
      "Batch 0/3: bce_loss: 0.6300898194313049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 7/10\n",
      "Epoch: 7/10\n",
      "Batch 0/3: bce_loss: 0.6354514956474304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 8/10\n",
      "Epoch: 8/10\n",
      "Batch 0/3: bce_loss: 0.6361994743347168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 9/10\n",
      "Epoch: 9/10\n",
      "Batch 0/3: bce_loss: 0.6231452822685242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  7.47it/s]\n"
     ]
    }
   ],
   "source": [
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    epoch_train_loss = []\n",
    "    epoch_valid_loss = []\n",
    "    print(f'Running epoch: {e}/{epochs-1}')\n",
    "    full_save_path = os.path.join(save_path, f'model_ep{e}.pt')\n",
    "    \n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x = batch['X'].to(device)\n",
    "        labels = batch['Y'].to(device)\n",
    "            \n",
    "        logits = classifier.forward(x)\n",
    "        loss = criterion(logits, labels.float().unsqueeze(dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        epoch_train_loss.append(loss.item())\n",
    "        \n",
    "        if idx % log_every == 0:\n",
    "            print(f'Epoch: {e}/{epochs}')\n",
    "            print(f'Batch {idx}/{len(train_dataloader)}: bce_loss: {loss}')\n",
    "    \n",
    "    \n",
    "    val_probs, val_labels, val_loss = evaluate_model(val_dataloader, classifier, criterion)\n",
    "    val_preds =  (torch.tensor(val_probs, dtype=torch.float32) > 0.5).long()\n",
    "    \n",
    "    val_weighted_auc = weighted_auc(val_labels, val_probs)\n",
    "    auc = metrics.roc_auc_score(val_labels, val_preds)\n",
    "    val_accuracy = metrics.accuracy_score(val_preds, val_labels)\n",
    "    val_f1_score = metrics.f1_score(val_preds, val_labels)\n",
    "    \n",
    "    train_history.append({'loss': np.mean(epoch_train_loss)})\n",
    "    val_history.append({'epoch': e, 'loss': val_loss, 'accuracy': val_accuracy, 'auc': auc,\n",
    "                        'weighted_auc': val_weighted_auc, 'f1_score': val_f1_score, 'save_path': full_save_path})\n",
    "    \n",
    "    torch.save(classifier.state_dict(), full_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: {'epoch': 8, 'loss': 0.7008912364641825, 'accuracy': 0.5666666666666667, 'auc': 0.425, 'weighted_auc': 0.3035714285714286, 'f1_score': 0.723404255319149, 'save_path': './saved_models/model_ep8.pt'}\n",
      "Loaded pretrained weights for efficientnet-b1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:02<00:00,  7.95it/s]\n"
     ]
    }
   ],
   "source": [
    "best_epoch = min(val_history, key=lambda hist: hist[target_metric])\n",
    "print(f'Best epoch: {best_epoch}')\n",
    "best_load_path = best_epoch['save_path']\n",
    "\n",
    "classifier = EfficientNetClassifier(model_name).to(device)\n",
    "classifier.load_state_dict(torch.load(best_load_path))\n",
    "\n",
    "df = make_submission(classifier, test_data_path, batch_size, device)\n",
    "df.to_csv(submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01a5a68734c94bce9f9325439a730712": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2a409302f9ac4f6fab318caddce0eb19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8824580b599f4335adcda5a852740a61": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8ae2fb82fd86459db7496b889387e7f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ffd96aac142248d996819459edf83f8d",
        "IPY_MODEL_e09458d7462a4d4b97f47a9629b463d5"
       ],
       "layout": "IPY_MODEL_01a5a68734c94bce9f9325439a730712"
      }
     },
     "9f0e9a48e0eb429eb1eea5f1c2aecc21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "b96982f378964597b9d0bda8277b3ab0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e09458d7462a4d4b97f47a9629b463d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8824580b599f4335adcda5a852740a61",
       "placeholder": "​",
       "style": "IPY_MODEL_2a409302f9ac4f6fab318caddce0eb19",
       "value": " 30.1M/30.1M [00:00&lt;00:00, 52.1MB/s]"
      }
     },
     "ffd96aac142248d996819459edf83f8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b96982f378964597b9d0bda8277b3ab0",
       "max": 31519111,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9f0e9a48e0eb429eb1eea5f1c2aecc21",
       "value": 31519111
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
